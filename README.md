# ğŸš€ AWS Data Lake: Pipeline para IngestÃ£o de Dados

Este repositÃ³rio contÃ©m o notebook sobre o aprendizado que desenvolvi durante o curso **AWS Data Lake: criando uma pipeline para ingestÃ£o de dados**. O objetivo foi construir uma pipeline de ingestÃ£o de dados externos, utilizando os principais serviÃ§os da AWS voltados para engenharia de dados e governanÃ§a.

---

## ğŸ“Œ Objetivo do Projeto

Construir uma pipeline de ingestÃ£o de dados baseada na base pÃºblica **Data Boston**, utilizando a AWS para estruturar um Data Lake em camadas (Bronze e Silver), garantindo seguranÃ§a e controle de acesso com boas prÃ¡ticas de cloud.

---

## ğŸ§° Tecnologias e Ferramentas

- **Amazon S3** â€“ Armazenamento de dados em camadas.
- **Amazon IAM** â€“ CriaÃ§Ã£o de usuÃ¡rios e controle de permissÃµes.
- **AWS Lake Formation** â€“ GovernanÃ§a de dados.
- **AWS Budgets e CloudWatch** â€“ Monitoramento e alertas de custo.
- **Python** â€“ Linguagem para manipulaÃ§Ã£o dos dados.
- **Boto3** â€“ ConexÃ£o com serviÃ§os AWS.
- **Urllib** â€“ ExtraÃ§Ã£o de dados externos via HTTP.
- **BytesIO** â€“ ManipulaÃ§Ã£o de arquivos em memÃ³ria.
- **Apache Parquet** â€“ Formato de arquivo otimizado.

---

## ğŸ§  O que foi aprendido

- CriaÃ§Ã£o de conta na AWS com boas prÃ¡ticas de seguranÃ§a (MFA).
- CriaÃ§Ã£o de alertas de custos para controle financeiro.
- CriaÃ§Ã£o e configuraÃ§Ã£o de usuÃ¡rios IAM.
- EstruturaÃ§Ã£o de um bucket S3 com camadas Bronze e Silver.
- Registro e configuraÃ§Ã£o do Lake Formation.
- Consumo de dados externos com `urllib`.
- Upload de dados em memÃ³ria com `BytesIO` e `boto3`.
- Escrita dos dados no formato `Parquet` para otimizar performance.
- DiferenÃ§a entre **Data Warehouse**, **Data Lake** e **Data Lakehouse**.

---
## ğŸ“ ObservaÃ§Ãµes

Este projeto Ã© um exercÃ­cio prÃ¡tico de um curso introdutÃ³rio, focado em entender os principais serviÃ§os da AWS aplicados Ã  ingestÃ£o de dados. Apesar de simples, foi essencial para fixar conceitos como o uso de buckets em camadas, permissÃµes com IAM e configuraÃ§Ãµes do Lake Formation, alÃ©m de treinar o consumo e upload de dados com Python.

---

## âœ¨ CrÃ©ditos

Curso: *AWS Data Lake: criando uma pipeline para ingestÃ£o de dados*  
Instrutor(a): Ana Hashimoto  
Plataforma: Alura
ğŸ” As credenciais da AWS sÃ£o carregadas de variÃ¡veis de ambiente para evitar exposiÃ§Ã£o no cÃ³digo. 

---

## ğŸ“ Links Ãšteis

- ğŸ“„ [Base de Dados Data Boston](https://data.boston.gov/dataset/311-service-requests)

---


